{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Введение. Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введение\n",
    "\n",
    "В этом модуле мы поговорим о следующем:\n",
    "\n",
    "функциях активации;\n",
    "инициализации;\n",
    "batch нормализации;\n",
    "dropout регуляции;\n",
    "градиентном спуске;\n",
    "стохастическом градиентном спуске;\n",
    "матричных операциях.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция активации нейрона определяет выходной сигнал, который в свою очередь определяется входным сигналом или набором входных сигналов. Функцию активации используют, чтобы получить выходные данные узла.\n",
    "\n",
    "Функции активации делятся на два типа:\n",
    "\n",
    "линейные функции активации;\n",
    "нелинейные функции активации.\n",
    "Сигмоида (Sigmoid) — возрастающая нелинейная функция, имеющая форму буквы «S». В нейронных сетях она используется потому, что позволяет усиливать слабые сигналы.\n",
    "\n",
    "Проблемы Sigmoid активации:\n",
    "\n",
    "Нейроны с сигмоидой могут насыщаться и приводить к угасающим градиентам.\n",
    "Не центрированы в нуле.\n",
    "Дорого вычислять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lh5.googleusercontent.com/GUebtq1pIUBCABD57tcVaroBte8t8R5u_d_67_gIVh8qviON0YzEX0Pa5B3M5xNX-dgzVfXRNanLKvF8UrapFjfzOC3LPfk6_tdm79K1wz64m5f8pwuUGNDnCQ0IS4z4yfrDO2jY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Tanh похожа на сигмоиду, но её преимущество состоит в том, что отрицательные входные данные будут отображаться строго отрицательными, а нулевые входные данные будут отображаться вблизи нуля.\n",
    "\n",
    "Характеристики Tanh активации:\n",
    "\n",
    "Центрирована в нуле.\n",
    "Но все ещё как сигмоида."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lh3.googleusercontent.com/ym0u8XXlxMKyFpBQTGgwFtEL9Ud5k4TBeHJu-4Nn3UqeMQx66ROD1LESkMaESnZ3sRIF0zhIBcmuVl6LLoqwWPmSEa6UBfq36GCIv3RuIBcFz4Xsf3Of08sgVV4S_i7bVetaABeB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU (rectified linear unit) является наиболее часто используемой функцией активации в мире, т. к. она используется практически во всех свёрточных нейронных сетях или для deep learning.\n",
    "\n",
    "Характеристики ReLU активации:\n",
    "\n",
    "Быстро считается.\n",
    "Градиенты не угасают при х > 0\n",
    "Не центрирована в нуле.\n",
    "Если не было активации — не будет обновления. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lh5.googleusercontent.com/nM2HgGZV2wpgjeZn6zzeSu6e5UJXFRXMifXJL8aAGy0nZOFm9xTiQjB3RYaNY2EGK8aZk7Bb8Gd61pjZfqdFaGdplV3bhJA_O2oJmxSDUYDOolcBsN2bWV9vhvN_FQ0R4R7vC-92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky ReLU является попыткой решить проблему выхода из строя ReLU.\n",
    "\n",
    "Характеристики Leaky ReLU активации:\n",
    "\n",
    "Всегда будут обновления.\n",
    "Примерно центрирована в нуле.\n",
    "а ≠ 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lh6.googleusercontent.com/MKlNLAjwKZfDyWIc2IW008IG6dcldDlPclIfNmwuFMlNPN7W7FX-zcxKyhwB7k4cSu3NkEewx_EBDRlgItpDwhlrQHnlqjkNrBjwT313BHgrPCbqwFG76YpqUVITLXJthElZHhsr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция активации ELU (Exponential Linear Unit), по результатам исследований, быстрее сводит к нулю и даёт более точные результаты.\n",
    "\n",
    "В отрицательной части аргументов использует экспоненту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lh4.googleusercontent.com/7KJzWSA8nungmCEveVUjb4EFil2My660uPnzP8u08VhcwtoC_ZXfeWuS0vH-VL8dMhRSAe1UVti3UfkgYlYVA9FOZt0MV6VOad-6A_daTzpJS3Jo_Bw3jVs0kSoeQJzFLyTCCL3h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Характеристики ELU:\n",
    "\n",
    "Примерно центрирована в нуле;\n",
    "Сходимость быстрее ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lh5.googleusercontent.com/TIutETjeAVISSvdtoQ_3-TEy7V3CopaPJLNno6aKJ0WGI-vfbHohQgtu4guNnbDmUcUD8lpJwXkXHNWo_ajIbuJolTNQFMOUsqVJ8CpeFx8tddnDXuio_MG4bm7Bebps1RK3iyr2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные материалы\n",
    "\n",
    "nlp\t\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статью «Fast and accurate deep network learning by exponential linear units (ELUs)»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1511.07289"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
