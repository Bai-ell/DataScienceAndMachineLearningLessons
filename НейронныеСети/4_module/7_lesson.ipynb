{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Стохастический градиентный спуск (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастический градиентный спуск (SGD)  является итерационным методом оптимизации с дифференцируемой целевой функцией, суть градиентного спуска — минимизировать функцию, делая небольшие шаги в сторону наискорейшего убывания функции.\n",
    "\n",
    "Плюсы SGD:\n",
    "\n",
    " позволяет быстрее делать шаги и быстрее сходиться;\n",
    " траектория становится более шумной, что помогает выпрыгивать из локальных оптимумов.\n",
    "RMSProp — адаптивный шаг. RMSProp — метод, в котором скорость обучения адаптируется для каждого из параметров.\n",
    "\n",
    "Градиентные методы медленно сходятся, если градиенты по разным параметрам в разном масштабе. Как их привести в один масштаб? Если мы делали несколько маленьких шагов по какой-то переменной, то можно увеличивать шаг, чтобы не делать оптимизацию слишком медленной по некоторым переменным. Т.е. мы делим скорость обучения для веса на скользящее среднее значение после градиентов для этого веса.\n",
    "\n",
    "Adam — Adaptive Moment Estimation. Adam — метод, в котором сочетаются инерция и адаптивность шага. Если перед вами стоит выбор вариации SGD, то выбирайте Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные материалы\n",
    "\n",
    "nlp\t\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статьи:\n",
    "\n",
    "«Why Momentum Really Works»;\n",
    "«An overview of gradient descent optimization algorithms»."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
