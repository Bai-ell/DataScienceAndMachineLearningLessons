{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Стохастический градиентный спуск (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастический градиентный спуск (SGD)  является итерационным методом оптимизации с дифференцируемой целевой функцией, суть градиентного спуска — минимизировать функцию, делая небольшие шаги в сторону наискорейшего убывания функции.\n",
    "\n",
    "Плюсы SGD:\n",
    "\n",
    " позволяет быстрее делать шаги и быстрее сходиться;\n",
    " траектория становится более шумной, что помогает выпрыгивать из локальных оптимумов.\n",
    "RMSProp — адаптивный шаг. RMSProp — метод, в котором скорость обучения адаптируется для каждого из параметров.\n",
    "\n",
    "Градиентные методы медленно сходятся, если градиенты по разным параметрам в разном масштабе. Как их привести в один масштаб? Если мы делали несколько маленьких шагов по какой-то переменной, то можно увеличивать шаг, чтобы не делать оптимизацию слишком медленной по некоторым переменным. Т.е. мы делим скорость обучения для веса на скользящее среднее значение после градиентов для этого веса.\n",
    "\n",
    "Adam — Adaptive Moment Estimation. Adam — метод, в котором сочетаются инерция и адаптивность шага. Если перед вами стоит выбор вариации SGD, то выбирайте Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные материалы\n",
    "\n",
    "nlp\t\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статьи:\n",
    "\n",
    "https://distill.pub/2017/momentum/;\n",
    "«An overview of gradient descent optimization algorithms»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матричные операции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Интерфейс прямого прохода\n",
    "\n",
    "Для прямого прохода интерфейс очень простой — это функция, которая принимает вход и генерирует выход, т.е. один вход и один выход.\n",
    "\n",
    "Обратный проход производится по графу из производных, т.е. когда мы идем по нему в обратную сторону, мы рассчитываем с помощью цепного правила производные по всем параметрам, которые есть в сети.\n",
    "\n",
    "Чтобы реализовать Backward pass для новой вершины нужно реализовать функцию, у которой есть входы —  то, что использовалось как вход сигмоиды во время прямого прохода. Т.к. необходимо считать производную в какой-то точке, сначала  back-propagation делает прямой проход, запоминает все значения аргументов, а потом использует их во время обратного прохода, чтобы рассчитать производные функций в нужных точках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lh5.googleusercontent.com/pxJn7e1eaLBvSpdEwQXx1dOZnM_0pD2w_amh71T8TQh6Fdbih6nwLjh1LlPzsC8drEwQy4sPP5aiv4xI9yiYdFhP0l7FfwRRi0qjvFzqCixOS6M0foWxAladvnCbz3G83GFw0hz5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полносвязный слой как произведение матриц \n",
    "\n",
    "Матричные операции используются часто, они реализованы быстро. Существуют такие пакеты как: CPU (BLAS) и GPU (cuBLAS), которые производят численные операции очень быстро, используя векторные операции процессора.\n",
    "\n",
    "Обратный проход\n",
    "\n",
    "Для обратного шага необходимо посчитать производную наших потерь (обычно это скалярное значение) по каждому весу, который мы использовали.\n",
    "\n",
    "Быстрая реализация в NumPy\n",
    "\n",
    "Производит матричные операции очень быстро, не на Python, и NumPy можно настроить таким образом, чтобы он использовал те же инструкции, что заложены в пакетах BLAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
