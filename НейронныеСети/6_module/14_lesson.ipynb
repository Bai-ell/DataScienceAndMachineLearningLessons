{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отказ от рекуррентных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“ATTENTION IS ALL YOU NEED” — это следующая идея после ATTENTION, предлагающая не использовать рекуррентные связи, а только лишь механизм внимания. Рассмотрим, как это можно сделать.\n",
    "\n",
    "Идея: Пусть вся входная цепочка без рекуррентных связей отображается в признаковое представление, по которому генерируется первое слово. Для генерации второго слова будет использоваться вся имеющаяся информация: входная цепочка и предыдущая история сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/a8decccb7183102184cd07ff276ae2c4/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL-1_10_модуль_23.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом генерируется вся цепочка.\n",
    "\n",
    "Отличие этого алгоритма в том, что инструмент, анализирующий входные и выходные слова, не использует рекуррентных временных связей, а сразу за один проход смотрит на всю цепочку, руководствуясь специальным видом ATTENTION.\n",
    "\n",
    "Такую идея использует архитектура TRANSFORMER. Перед тем, как переходить к подробному её рассмотрению, познакомимся с механизмом Self-Attention.\n",
    "\n",
    "SELF-ATTENTION\n",
    "\n",
    "Self-Attention — это способ в самой цепочке определить, как соотносятся друг с другом её элементы. \n",
    "\n",
    "Если в случае ATTENTION используется информация и о выходной цепочке, то Self-Attention позволяет сгенерировать векторное представление элементов входной последовательности, используя информацию о всей цепочке. Механизм Self-Attention достаточно нетривиальный и заслуживает дополнительного изучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные материалы\n",
    "\n",
    "nlp\t\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статью «Self-Attention Mechanisms in Natural Language Processing»https://alibaba-cloud.medium.com/self-attention-mechanisms-in-natural-language-processing-9f28315ff905"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
