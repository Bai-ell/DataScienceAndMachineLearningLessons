{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекуррентные нейронные сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граф, соответствующий простой рекуррентной нейронной сети, можно представить в следующем виде:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/515902d9783b2726bd781797e95755f1/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp6_1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помним, что стрелочками обозначены линейные слои (например, умножение на матрицу, сложение с вектором смещения или взятие функции активации). Здесь добавляется стрелка, обозначающая соединение вектора Ht с самим собой в разные моменты времени. То есть вычисление каждого Ht будет зависеть не только от Xt, но и от Ht-1. Поэтому при вычислении Ht добавляется дополнительное слагаемое WhHt-1  — произведение матрицы весов на вектор промежуточного скрытого состояния с предыдущего момента времени. Таким образом в векторе H накапливается информация о всей последовательности.\n",
    "\n",
    "UNFOLDING\n",
    "\n",
    "Полезно рассматривать такую сеть в виде развёрнутой нейронной сети — такая операция называется UNFOLDING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/893db96ea4ceaf871cbf1f4868078403/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp7.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом представлении (см. рисунок) сеть с циклической зависимостью представляется в виде обычной сети прямого распространения с несколькими входными и выходными векторами. Но важное отличие от абстрактной сети прямого распространения в том, что параметры этой модели одни и те же в разные моменты времени. То есть все копии нейронной сети по моментам времени — это одна и та же нейронная сеть, одни и те же параметры. По этой последовательности прогоняем входной элемент и последовательно получаем выходные значения.\n",
    "\n",
    "Как обучать такого рода сеть?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример: пусть есть последовательность на входе и для простоты такая же последовательность на выходе (такого же размера и с некоторым соответствием «один в один»: X1 в Y1 и т. д.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/893db96ea4ceaf871cbf1f4868078403/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp7.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/2f03a26cb5e80246b4bc8b2496f2fbe7/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp8.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждый выходной элемент последовательности мы можем навесить Loss (функцию потерь). То есть выход из нейронной сети мы хотим «притянуть» к известному нам правильному значению. Loss зависит от параметров сети, которые в свою очередь зависят от X1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, если мы применяем обратное распространение ошибки, градиент будет протекать от Loss к X1. От остальных элементов последовательности ошибка и градиент никак не зависят. Поэтому это эквивалентно обратному распространению ошибки в обычных нейронных сетях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/d2148fd65df07735c27ee5d3ac6aa25e/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, если мы хотим посчитать функцию потерь, например, для третьего элемента, то возникнет более сложная зависимость и Loss будет зависеть не только от X3, но и от X1 и X2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/c389eb22613e51ea292e6937a63969fd/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp10.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, происходит не просто обратное распространение ошибки по слоям, а добавляется временнОе измерение. Градиент необходимо протаскивать во время, в те позиции, от которых зависит финальная ошибка. Такой алгоритм будет называться Backpropagation through time («обратное распространение ошибки сквозь время»). Градиент как бы растекается по двум направлениям: по сети (по её глубине) и по времени назад (в случае, если сеть однонаправленная)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/dd489bc2034ae39f95c341aba5d0a2b2/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp11.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо поместить всё в один Loss, оптимизируя финальную ошибку по всем элементам выходной последовательности, то все Loss'ы суммируются, а градиент суммы вычисляется как сумма градиентов. Здесь потребуется правильно расписать дифференцирование сложной функции, грамотно учесть все зависимости. Главное, нужно учесть, что веса делятся между разными позициями времени. По тому же принципу, что и для построения обратного распространения ошибки для обычных сетей, можно выписать все градиенты для такого типа рекуррентных сетей.\n",
    "\n",
    "Описанный алгоритм простых рекуррентных нейронных сетей уже хорош для анализа последовательностей, но у  него есть некоторые проблемы, которые мы предлагаем разобрать в следующем разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
