
# Методы оптимизации в машинном обучении

## 1. SGD (Stochastic Gradient Descent)
**Описание**:  
SGD — базовый метод оптимизации, где веса обновляются на основе градиента функции потерь для одного случайно выбранного образца (или мини-батча) данных.

**Формула**:  
\[
\theta_{t+1} = \theta_t - \eta \cdot \nabla J(\theta)
\]  
Где:  
- \( \theta_t \) — текущие веса модели.  
- \( \eta \) — скорость обучения (learning rate).  
- \( \nabla J(\theta) \) — градиент функции потерь.

**Особенности**:  
- Использует случайность (stochastic), выбирая случайный поднабор данных для расчета градиента на каждой итерации.
- Уменьшает вычислительные затраты по сравнению с Batch Gradient Descent.

**Преимущества**:  
- Простота реализации.  
- Эффективен для больших наборов данных.  
- Может выйти из локальных минимумов благодаря шуму в обновлениях.

**Недостатки**:  
- Колебания из-за шума (градиенты могут быть не точны).  
- Зависимость от выбора скорости обучения (\( \eta \)).

---

## 2. Momentum (Градиент с моментумом)
**Описание**:  
Модификация SGD, которая добавляет инерцию, чтобы ускорить обучение в нужном направлении и уменьшить колебания.

**Формулы**:  
1. Скорость изменения:  
\[
v_t = \gamma \cdot v_{t-1} + \eta \cdot \nabla J(\theta)
\]  
2. Обновление весов:  
\[
\theta_{t+1} = \theta_t - v_t
\]  
Где:  
- \( v_t \) — скорость (momentum).  
- \( \gamma \) — коэффициент моментума (обычно \( 0.9 \)).

**Особенности**:  
- Скорость зависит как от текущего градиента, так и от накопленной скорости.

**Преимущества**:  
- Быстрая сходимость.  
- Уменьшает колебания и шум.

**Недостатки**:  
- Требует настройки \( \gamma \).

---

## 3. AdaGrad (Adaptive Gradient Descent)
**Описание**:  
Метод адаптирует скорость обучения для каждого параметра, уменьшая её для часто изменяемых параметров и увеличивая для редко изменяемых.

**Формула**:  
\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \cdot \nabla J(\theta)
\]  
Где:  
- \( G_t \) — сумма квадратов всех прошлых градиентов для параметра.  
- \( \epsilon \) — малое число для избежания деления на 0.

**Особенности**:  
- Использует историю градиентов.

**Преимущества**:  
- Хорошо работает с разреженными данными.

**Недостатки**:  
- Скорость обучения может стать слишком маленькой.

---

## 4. RMSProp (Root Mean Square Propagation)
**Описание**:  
Модификация AdaGrad, которая использует экспоненциальное скользящее среднее вместо суммирования прошлых градиентов.

**Формулы**:  
1. Обновление градиентов:  
\[
v_t = \beta \cdot v_{t-1} + (1 - \beta) \cdot (\nabla J(\theta))^2
\]  
2. Обновление весов:  
\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} \cdot \nabla J(\theta)
\]  
Где:  
- \( \beta \) — коэффициент затухания (обычно \( 0.9 \)).

**Особенности**:  
- Балансирует адаптивную скорость обучения.

**Преимущества**:  
- Эффективен для рекуррентных нейронных сетей.  
- Подходит для шумных данных.

---

## 5. Adam (Adaptive Moment Estimation)
**Описание**:  
Комбинирует RMSProp и Momentum, учитывая как среднее значение градиентов, так и средний квадрат градиентов.

**Формулы**:  
1. Вычисление моментов:  
\[
m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot \nabla J(\theta)
\]  
\[
v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot (\nabla J(\theta))^2
\]  
2. Коррекция моментов:  
\[
\hat{m_t} = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v_t} = \frac{v_t}{1 - \beta_2^t}
\]  
3. Обновление весов:  
\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t}} + \epsilon} \cdot \hat{m_t}
\]

**Преимущества**:  
- Хорошая производительность "из коробки".  
- Быстрая сходимость.  
- Устойчив к шуму.

**Недостатки**:  
- Может не достигать глобального минимума.

---

## 6. Nadam (Nesterov-accelerated Adaptive Moment Estimation)
**Описание**:  
Расширение Adam, включающее **Nesterov Momentum**, чтобы предвидеть будущее направление градиента.

**Формула**:  
Добавляет предсказание изменения градиента перед шагом обновления:  
\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t}} + \epsilon} \cdot \left( \beta_1 \cdot \hat{m_t} + \frac{(1 - \beta_1)}{\nabla J(\theta)} \right)
\]

**Преимущества**:  
- Быстрая и точная сходимость.

---

## 7. Adadelta
**Описание**:  
Модификация AdaGrad, которая ограничивает суммирование прошлых градиентов, чтобы скорость обучения не падала слишком сильно.

**Формулы**:  
1. Скользящее среднее градиента:  
\[
v_t = \beta \cdot v_{t-1} + (1 - \beta) \cdot (\nabla J(\theta))^2
\]  
2. Обновление весов:  
\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} \cdot \nabla J(\theta)
\]

**Преимущества**:  
- Эффективен для разреженных данных.  
- Не требует ручной настройки скорости обучения.

---

## 8. L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)
**Описание**:  
Метод второго порядка, который использует приближение Гессиана (матрицы вторых производных) для нахождения минимума.

**Преимущества**:  
- Хорош для малых моделей.  
- Высокая точность.

**Недостатки**:  
- Неэффективен для больших данных.
