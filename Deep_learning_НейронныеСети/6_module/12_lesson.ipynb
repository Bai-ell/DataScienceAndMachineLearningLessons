{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Механизм внимания. ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемы Encoder-decoder\n",
    "\n",
    "Основная проблема Encoder-decoder в том, что вся информация о входной цепочке хранится в одном векторе. Проблема эта возникает даже несмотря на использование LSTM и GRU ячеек, которые следят за тем, чтобы длинные цепочки так же хорошо обрабатывались и информация накапливалась в промежуточном состоянии. Проблема связана с тем, что элементы, которые стояли в начале предложения и те, что стояли в конце, воспринимаются сетью по-разному. Иными словами, начало и конец предложения используются неравномерно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/29f0b66f673d4a8cc22e6855b141c8e2/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL-1_10_модуль_15_1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта проблема может быть исправлена с помощью двунаправленных рекуррентных нейронных сетей, которые действительно часто используются в качестве encoder. Два вектора, полученных в ходе работы двунаправленной сети, суммируются. Таким образом, в каждый момент времени сеть помнит, что происходило до и после текущего момента.\n",
    "\n",
    "Но такой способ тем не менее не решает всей проблемы, и какие-то элементы всё-таки находятся ближе, а какие-то дальше от финального закодированного состояния. Как можно эту архитектуру улучшить?\n",
    "\n",
    "Механизм внимания (ATTENTION)\n",
    "\n",
    "В предыдущем материале архитектура Encoder-decoder выглядела следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/bbdf8160e06647364540599335b96e79/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL-1_10_модуль_16.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данную архитектуру легко можно улучшить, применив Механизм внимания (ATTENTION). ATTENTION предлагает смотреть на всю входную цепочку сразу, а не только на последний кодированный элемент, чтобы предсказать один выходной элемент последовательности.  Предлагается закодированный входной вектор подавать в качестве входа в decoder и кроме того агрегировать в него информацию со всех элементов входной цепочки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/bfe83a4e181f55d41692d31ba1d1d37b/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL-1_10_модуль_17.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выходная цепочка с помощью такого механизма генерируется как линейная комбинация скрытых векторов с весами, которые соответствуют тому, насколько важен вклад того или иного входного слова в генерацию текущего выходного слова. \n",
    "\n",
    "В нашем примере первым словом скорее всего будет «привет». Механизм внимания сообщает, что с бОльшим весом нужно взять информацию с первого слова «hello», потому что скорее всего именно оно соответствует первому слову в выходной цепочке. Используя эту информацию, генерируем первое слово «привет», которое далее подаём на вход в сеть, а в качестве второго входа — механизм внимания на все элементы последовательности, вновь определяющий веса каждого слова входной цепочки. И т.д.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/3f5e0c4f1df907cb9b802ccf4a06f832/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL-1_10_модуль_18.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной вопрос: как вычислять веса? Как понять, с каким весом необходимо взять то или иное слово входной последовательности?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
