{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  TRANSFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMER — это достаточно сложная архитектура с множеством вариаций. Мы рассмотрим упрощённую схему.\n",
    "\n",
    "TRANSFORMER применяется в задачах машинного перевода, чат ботов и пр., по качеству обыгрывая рекуррентные нейронные сети. Рекуррентные нейронные сети с LSTM, GRU и механизмом внимания показывают хороший результат и применяются в настоящее время во многих бизнес задачах, но для улучшения качества работы сообществом всё больше применяется именно архитектура TRANSFORMER, как качественная мощная модель. Рассмотрим упрощённую схему на примере.\n",
    "\n",
    "Итак, пусть в настоящее время в модель TRANSFORMER подаётся входная цепочка и вся сгенерированная ранее информация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/3b47b42f2651cb0baf6cd605698446e8/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL-1_10_модуль_24_1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attention анализирует информацию о входной цепочке и Encoder кодирует признаковое представление входной цепочки. В Decoder подаётся ранее сгенерированная выходная цепочка, к которой так же был применён механизм Self-Attention, и информация из encoder. Всё это пропускается через механизм Attention, после чего может быть пропущено ещё через какие-то слои. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/fbb75a62e92e89f63fc892c0c0d37aa5/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL-1_10_модуль_25.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, предсказанное на выходе слово будет содержать информацию о входной и о выходной цепочке.\n",
    "\n",
    "TRANSFORMER является сложной современной архитектурой и на всех задачах типа машинного перевода и чат ботов показывает самые высокие результаты. Рекуррентные нейронные сети тоже продолжают быть использованными, т.к. порой проще обучаются, а на маленьких задачах могут показать более хорошие результаты. Будем иметь в виду обе архитектуры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные материалы\n",
    "\n",
    "nlp\t\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статью «Азбука ИИ: «Рекуррентные нейросети» от МФТИ\n",
    "https://nplus1.ru/material/2016/11/04/recurrent-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
