{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети в NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом модуле мы поговорим о нейронных сетях для решения задач обработки естественного языка (NLP).\n",
    "\n",
    "Речь пойдёт не только о рекуррентных нейронных сетях, но и о других типах, более современных, которые также используются в NLP. Начнём с повторения уже известного нам материала:\n",
    "\n",
    "Нейронные сети прямого распространения\n",
    "\n",
    "К этому классу можно отнести все сети, рассмотренные нами ранее. В таких полносвязных нейронных сетях на вход подаётся вектор и на выходе также получается вектор, т.о. информация распространяется только в одном направлении от входа к выходу по принципу «один объект на входе — один объект на выходе» и на этом обработка заканчивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычная полносвязная нейронная сеть состоит из:\n",
    "\n",
    "входного вектора X;\n",
    "матриц весов B1, W1;\n",
    "вектора скрытого состояния H;\n",
    "иных матриц B2, W2;\n",
    "выходного вектора Y.\n",
    "Таким образом, имеем простое линейное преобразование с некоторой нелинейной функцией H."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/ffcb208a24e250fbc27d0a2eedbe818b/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае можем представить этот процесс как многослойную нейронную сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/db31398133def3ec5d2c5e5f82996129/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сами слои — линейные преобразования, например, умножение на матрицу и взятие функции активации — удобно обозначать на рисунке стрелочками. Далее будем использовать обозначение: стрелочка = слой.\n",
    "\n",
    "BACKPROPAGATION\n",
    "\n",
    "Нейронные сети такого типа обучаются с помощью обратного распространения ошибки (BACKPROPAGATION). Входной объект мы прогоняем через нейронную сеть и получаем выход, по которому оцениваем ошибку и по ней считаем градиент и применяем его к параметрам. Градиент ошибки считается по всем параметрам модели, параметрам  и весам каждого слоя. Необходимо понять, как зависит ошибка от параметром того или иного слоя. Для этого используется правило дифференцирования сложной функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/556d3bd7e68c2ab7f4a63b92d88b2a06/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp3_1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка последовательностей\n",
    "\n",
    "Для обработки последовательностей можно использовать нейронные сети прямого распространения и первое, что нужно сделать для этого — извлечь векторные представления слов и далее каждый вектор прогнать через нейронную сеть, получить ответ. Основная проблема в таком алгоритме — информация во времени никак не связана, слова подаются последовательно , но нет никакой информации о том, что было перед текущим словом или после текущего слова. Можно решить эту проблему, пройдя свёрточной нейронной сетью по входным векторам, уловив локальные зависимости.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/18da37c17e96ad0b758459431807205e/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp4.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекуррентные нейронные сети\n",
    "\n",
    "Рекуррентные нейронные сети — это мощный способ обработки последовательностей. Основная идея рекуррентных сетей в том, что информация накапливается во внутреннем представлении нейронной сети. Таким образом, прогон элемента уже имеет какое-то представление о всей предыдущей последовательности.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/68b1225f83df355fb446d2f13af378e8/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/nlp5.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
