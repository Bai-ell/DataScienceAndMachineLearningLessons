{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как обучить нейронную сеть?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети обучаются с помощью обучения с учителем или на примерах. Для этого нам нужна обучающая выборка, которая состоит из пар входной объект – выходной объект. Мы подаём эту обучающую выборку в процесс обучения, который состоит в том, чтобы найти такие параметры модели W, чтобы наша нейронная сеть предсказывала правильно те самые ответы, которые мы уже знаем.\n",
    "\n",
    "Таким образом мы решаем задачу минимизации или задачу оптимизации: мы минимизируем ошибку на тех примерах, которые есть в нашей обучающей выборке.\n",
    "\n",
    "Ошибку можно записать по-разному:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/07277b4c296443c85ebb369babf51760/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL_1_модуль_10_2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь она записана как разница между правильным ответом D и предсказанием сети GW(Z). От этой разницы мы берём норму и получаем скаляр, который мы хотим минимизировать по всей обучающей выборке. Затем мы ищем такие веса (нижнее соотношение W* ), которые минимизируют эту ошибку. \n",
    "\n",
    "Как решать задачу оптимизации \n",
    "\n",
    "Есть различные способы. В теории оптимизации есть такой известный алгоритм как градиентный спуск. Смысл объясним на примере. \n",
    "\n",
    "Допустим, у вас есть какая-то функция (здесь представлена одномерная функция, но в общем случае она может быть и многомерной), и вы хотите найти её минимум. Вы стоите в некоторой точке, и вам нужно понять, в какую сторону вам с этой точки нужно сдвинуться, чтобы приблизиться к минимуму. Есть вектор, который называется градиент. И этот вектор смотрит в сторону возрастания функции. Поэтому градиент со знаком минус смотрит в сторону убывания функции.\n",
    "\n",
    "Подобный алгоритм предлагает двигаться в сторону антиградиента и таким образом приближаться к локальному минимуму. \n",
    "\n",
    "Итак, мы вычислили градиент и с некоторым параметром α, который ещё называют Learning rate (скорость обучения) мы этот антиградиент прибавляем к нашим текущем весам, и так получается итерационное движение к минимуму. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/4480010d90260a7aac5504fcff8ca27b/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL_1_модуль_11.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае нейронных сетей используется небольшая модификация — стохастический градиентный спуск. Отличие от предыдущего градиентного спуска в том, что мы не вычисляем градиент сразу на всех образцах нашей выборки, а только на одном образце за одну итерацию или на группе образцов.\n",
    "\n",
    "Как вычислить градиент?\n",
    "\n",
    "Это вектор или даже некоторый тензор, размерность которого совпадает с размерностью всех наших параметров обучаемых."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/26eb057a00c5896bffe1bf315d3cdae8/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL_1_модуль_12.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент может особенно зависеть от слоёв, которые далеки от той самой ошибки, от которой мы считаем градиент. Например, у нас ошибка зависит от параметров первого слоя очень сложным образом, и тем не менее мы всё равно можем вычислить градиент, с помощью алгоритма обратного распространения ошибки, который заключается в том, что мы используем  правила дифференцирования сложной функции.\n",
    "\n",
    "Нейронная сеть, даже если она представляет собой сложную функцию, на самом деле просто композиция каких-то маленьких простых вещей. Например, умножили на матрицу, прибавили вектор, взяли поэлементную матрицу и так далее. Используя такое дифференцирование сложной функции можно узнать, как наша ошибка зависела от параметров второго слоя через это цепное правило. Именно таким образом можно вычислить градиент по любому весу и по всем весам. Это обратное распространение ошибки или ещё называют backpropagation.\n",
    "\n",
    "Для каких задач применять?\n",
    "\n",
    "классификация;\n",
    "регрессия;\n",
    "машинное зрение.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
