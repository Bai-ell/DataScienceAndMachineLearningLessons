{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Трюк 1: EXPLORATION vs EXPLOITATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущем материале мы руководствовались текущей политикой на данном этапе, выбирая, при каком действии значение Q-максимально. Такой подход называется exploitation, т.е. мы эксплуатируем оптимальную стратегию, которой до текущего момента научились. Это может приводить к ошибочным действиям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/2c03b13aa4ad994e18812870a0e6bbc4/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL_11_модуль_27.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы избежать ошибок, обратимся к exploration — исследованию новых путей, игнорирую политику.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/e7478ce62e304dce425513065059b458/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL_11_модуль_28.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совмещая два предыдущих подхода получаем эпсилон-жадный подход (eps-greedy), по которому с разной вероятностью выбираем случайное (новое) действие и действие, которое максимизирует Q-функцию. Т.е. в большинстве случаев игра идёт согласно оптимальной политике, но с маленькой вероятностью свершаются случайные действия для проверки новых траекторий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/de64e58a99b5e35f5fab2d67faeccb3e/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL_11_модуль_29.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
