{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16/24   Трюк 2: EXPERIENCE REPLAY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По каждой новой цепочке s, a, r, s' в предыдущем материале модуля мы обновляли веса модели. Цикл обучения представлял собой цикл по эпизодам, а внутри него цикл по шагам. \n",
    "\n",
    "Две соседние цепочки внутри эпизода окажутся в большой степени скоррелированными, что является недостатком.\n",
    "\n",
    "Решить проблему корреляции можно использованием Replay buffer, который представляет собой историю предыдущих состояний, действий, наград. Грубо говоря, это список цепочек s, a, r, s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lms-cdn.skillfactory.ru/assets/courseware/v1/81bc9d527a0ecd7ef22a2326fd2faf00/asset-v1:SkillFactory+MLDL+ALWAYS+type@asset+block/DL_11_модуль_32_1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение проходит на небольшом количестве цепочек в mini-batch, на котором проходит обновление весов. Это позволяет проводить обновление на старых цепочках. \n",
    "\n",
    "Совместив EXPERIENCE REPLAY и Q-LEARNING: ОБУЧЕНИЕ, получим мощный Q-LEARNING."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
